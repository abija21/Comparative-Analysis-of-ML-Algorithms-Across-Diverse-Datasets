{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5015dc6f-96cb-4398-9a3e-a4dd2aa44fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cce7457-59e4-41da-9dfc-183856eb3a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding for categorical attributes\n",
    "def one_hot_encode(df):\n",
    "    return pd.get_dummies(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05f9119a-fb53-4861-a0a0-1d78b37143e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Adult Income Dataset\n",
    "adult = pd.read_csv('adult.data', header=None)\n",
    "sampled_data = adult.sample(n=1000, random_state=42) # Randomly select 1000 instances\n",
    "df_encoded = one_hot_encode(sampled_data)\n",
    "X = df_encoded.iloc[:, :-1].values.tolist()  # Convert DataFrame to list of lists\n",
    "y = df_encoded.iloc[:, -1].values.tolist()   # Convert DataFrame to list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3eff34cd-e9e0-49d3-af93-20f95b894bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute Euclidean distance\n",
    "def euclidean_distance(instance1, instance2):\n",
    "    # print(instance1, instance2)\n",
    "    sq_difference = (instance1 - instance2) ** 2\n",
    "    sum_of_sq_difference = np.sum(sq_difference)\n",
    "    euclidean_distance = np.sqrt(sum_of_sq_difference)\n",
    "    # print(euclidean_distance)\n",
    "    return euclidean_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02a72b91-e30a-44b8-a7d7-07433929537a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get nearest neighbors\n",
    "def get_neighbors(train, test_row, k):\n",
    "    distances = []\n",
    "    for train_row in train:\n",
    "        dist = euclidean_distance(test_row, train_row[:-1])\n",
    "        distances.append((train_row, dist))\n",
    "    distances.sort(key=lambda tup: tup[1])\n",
    "    neighbors = []\n",
    "    for i in range(k):\n",
    "        neighbors.append(distances[i][0])\n",
    "    return neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4beaae95-9864-4f75-af4b-2a018f67e5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict the class\n",
    "def predict_classification(train, test_row, k):\n",
    "    neighbors = get_neighbors(train, test_row, k)\n",
    "    output_values = [row[-1] for row in neighbors]\n",
    "    prediction = max(set(output_values), key=output_values.count)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80b3f133-8f62-4215-b5d7-deb1495fb1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified split function\n",
    "def stratified_kfold(X, y, k):\n",
    "    from collections import defaultdict\n",
    "    folds = [([], []) for _ in range(k)]\n",
    "    label_indices = defaultdict(list)\n",
    "\n",
    "    # Organize indices by label\n",
    "    for idx, label in enumerate(y):\n",
    "        label_indices[label].append(idx)\n",
    "\n",
    "    # Distribute indices of each label roughly evenly across the folds\n",
    "    for indices in label_indices.values():\n",
    "        np.random.shuffle(indices)\n",
    "        for fold_idx, idx in enumerate(indices):\n",
    "            # folds[fold_idx % k][0].append(X.iloc[idx])  # for parkinson & titanic\n",
    "            # folds[fold_idx % k][1].append(y.iloc[idx])\n",
    "            folds[fold_idx % k][0].append(X[idx])  # for loan dataset\n",
    "            folds[fold_idx % k][1].append(y[idx])\n",
    "    # Shuffle each fold to mix labels\n",
    "    for fold in folds:\n",
    "        combined = list(zip(*fold))\n",
    "        np.random.shuffle(combined)\n",
    "        fold[0][:], fold[1][:] = zip(*combined)\n",
    "\n",
    "    return folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46aaedad-3298-44f3-827f-80e768ef52eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the model over a range of k values\n",
    "k_values = range(1, 52, 5)\n",
    "# average_training_accuracies = []\n",
    "average_testing_accuracies = []\n",
    "# average_training_f1_scores = []\n",
    "average_testing_f1_scores = []\n",
    "accuracy_std_devs = []\n",
    "f1_score_std_devs = []\n",
    "\n",
    "# Perform manual stratified K-fold\n",
    "folds = stratified_kfold(X, y, 10)\n",
    "\n",
    "for k in k_values:\n",
    "    # training_accuracies = []\n",
    "    testing_accuracies = []\n",
    "    # training_f1_scores = []\n",
    "    testing_f1_scores = []\n",
    "\n",
    "    for i in range(10):\n",
    "        X_test, y_test = folds[i]\n",
    "        X_train = [item for idx, fold in enumerate(folds) if idx != i for item in fold[0]]\n",
    "        y_train = [item for idx, fold in enumerate(folds) if idx != i for item in fold[1]]\n",
    "\n",
    "        # Normalize the features\n",
    "        scaler = MinMaxScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "        # Prepare data for k-NN (add labels back to features for training)\n",
    "        train_data = np.column_stack((X_train_scaled, y_train))\n",
    "        test_data = np.column_stack((X_test_scaled, y_test))\n",
    "\n",
    "        # Make predictions for each test instance and evaluate\n",
    "        y_pred_train = [predict_classification(train_data, row[:-1], k) for row in train_data]\n",
    "        y_pred_test = [predict_classification(train_data, row[:-1], k) for row in test_data]\n",
    "\n",
    "        # training_accuracies.append(accuracy_score(y_train, y_pred_train))\n",
    "        testing_accuracies.append(accuracy_score(y_test, y_pred_test))\n",
    "        # training_f1_scores.append(f1_score(y_train, y_pred_train, average='macro'))\n",
    "        testing_f1_scores.append(f1_score(y_test, y_pred_test, average='macro'))\n",
    "\n",
    "    # Storing average scores\n",
    "    # average_training_accuracies.append(np.mean(training_accuracies))\n",
    "    average_testing_accuracies.append(np.mean(testing_accuracies))\n",
    "    # average_training_f1_scores.append(np.mean(training_f1_scores))\n",
    "    average_testing_f1_scores.append(np.mean(testing_f1_scores))\n",
    "\n",
    "    # Calculate and append the standard deviation for the current k\n",
    "    accuracy_std_devs.append(np.std(testing_accuracies))\n",
    "    f1_score_std_devs.append(np.std(testing_f1_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "88ba6321-a238-4bf1-a122-f0746f744527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary Statistics:\n",
      "k: 1, Accuracy: 0.9321, F1 Score: 0.9089\n",
      "k: 6, Accuracy: 0.9110, F1 Score: 0.8712\n",
      "k: 11, Accuracy: 0.9261, F1 Score: 0.8968\n",
      "k: 16, Accuracy: 0.9110, F1 Score: 0.8731\n",
      "k: 21, Accuracy: 0.9291, F1 Score: 0.8991\n",
      "k: 26, Accuracy: 0.9300, F1 Score: 0.9004\n",
      "k: 31, Accuracy: 0.9370, F1 Score: 0.9120\n",
      "k: 36, Accuracy: 0.9260, F1 Score: 0.8950\n",
      "k: 41, Accuracy: 0.9241, F1 Score: 0.8941\n",
      "k: 46, Accuracy: 0.9160, F1 Score: 0.8809\n",
      "k: 51, Accuracy: 0.9110, F1 Score: 0.8729\n"
     ]
    }
   ],
   "source": [
    "print(\"Summary Statistics:\")\n",
    "for k, accuracy, f1 in zip(k_values, average_testing_accuracies, average_testing_f1_scores):\n",
    "    print(f\"k: {k}, Accuracy: {accuracy:.4f}, F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822546e0-126d-4dcb-a697-874d06bfe4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Accuracy\n",
    "plt.figure(figsize=(10, 5))\n",
    "# plt.plot(k_values, average_training_accuracies, label='Training Accuracy', marker='o', color='blue')\n",
    "plt.errorbar(k_values, average_testing_accuracies, yerr=accuracy_std_devs, label='Accuracy', marker='s', color='grey',\n",
    "             linestyle='-', fmt='o', capsize=5)\n",
    "plt.title('k-NN Accuracy for Varying k')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0075eee3-98e8-4a02-9b68-e60633a5b1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting F1 Score\n",
    "plt.figure(figsize=(10, 5))\n",
    "# plt.plot(k_values, average_training_f1_scores, label='Training F1 Score', marker='o', color='blue')\n",
    "plt.errorbar(k_values, average_testing_f1_scores, yerr=f1_score_std_devs, label='F1 Score', marker='s', color='grey',\n",
    "             linestyle='-', fmt='o', capsize=5)\n",
    "plt.title('k-NN F1 Score for Varying k')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base]",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
